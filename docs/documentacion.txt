Documentación Técnica Avanzada: Generador de Ideas para Contenido Web
Versión: 2.1 (Revisada)
Fecha: 2025-01-31
Autor: [Tu Nombre o Equipo]
Objetivo: Automatizar la generación de ideas de contenido web basadas en análisis de un sitemap y fuentes como Google Autocomplete, People Also Ask (PAA) y Google Trends.

Tabla de Contenidos
Visión General
Objetivos y Alcance
Requerimientos Técnicos
Arquitectura del Proyecto
Estructura de Directorios
Diagrama de Componentes
Flujo de Datos
Diseño e Implementación
Módulo de Configuración (config.py)
Scrapers y Procesamiento
Interfaz Web con Flask
Persistencia y Cache
Manejo de Errores y Seguridad
Pruebas y Validación
Despliegue y Operación
Mejoras Futuras y Roadmap
Instrucciones para el "Developer IA"
Referencias y Recursos
1. Visión General
El Generador de Ideas para Contenido Web es una aplicación escrita en Python que:

Analiza el contenido de un sitio leyendo su sitemap.xml.
Obtiene nuevas palabras clave desde Google Autocomplete, People Also Ask (con Playwright) y Google Trends (con Pytrends).
Compara las nuevas palabras clave con el contenido existente (usando un mecanismo de coincidencia parcial).
Presenta resultados (nuevas oportunidades de contenido) mediante una interfaz web en Flask.
Permite añadir manualmente nuevas ideas de contenido.
2. Objetivos y Alcance
Objetivos Principales
Extracción de Contenido Existente: Parsear el sitemap y, opcionalmente, scrapear cada URL para obtener títulos y descripciones.
Generación de Keywords: A partir de palabras semilla, usar Google Autocomplete, PAA y Trends para descubrir términos relevantes.
Comparación (Fuzzy Matching): Determinar si una keyword ya está cubierta por el contenido del sitio.
Interfaz Simple: Mostrar todo en una aplicación web local con posibilidades de filtrar, añadir manualmente, etc.
Alcance
Scraping Responsable (con rate limiting y, si es necesario, rotación de user-agents).
Persistencia Ligera con SQLite para el contenido y archivos CSV (o JSON) para almacenar las nuevas keywords.
Modularidad: Posibilidad de agregar más fuentes de datos en el futuro.
3. Requerimientos Técnicos
Python 3.10+
Librerías Clave:
requests, beautifulsoup4, playwright, pytrends, flask, python-dotenv, difflib, logging
Base de Datos: SQLite (nativo de Python).
Framework Web: Flask.
Despliegue: Local o en un servicio que soporte aplicaciones Flask (Railway, Render, Docker, etc.).
4. Arquitectura del Proyecto
4.1. Estructura de Directorios
plaintext
Copiar
Editar
keyword_idea_generator/
├── app.py                   # Punto de entrada (Flask + rutas).
├── config.py                # Configuración global.
├── scraper/                 # Módulos de scraping.
│   ├── sitemap_parser.py        # Extrae URLs, títulos y descripciones del sitemap.
│   ├── google_autocomplete.py   # Sugerencias de Google Autocomplete.
│   ├── google_paa.py            # "People Also Ask" vía Playwright.
│   └── google_trends.py         # Datos de Google Trends con Pytrends.
├── data/                    # Almacenamiento de datos.
│   ├── existing_content.db      # SQLite (contenido actual).
│   └── keywords/               # Carpeta con resultados CSV o JSON.
│       ├── autocomplete.csv
│       ├── paa.csv
│       └── trends.csv
├── templates/               # Plantillas HTML (Flask).
│   ├── base.html
│   ├── index.html
│   ├── results.html
│   └── add_keyword.html
├── static/                  # Archivos estáticos (CSS, JS, imágenes).
│   ├── css/
│   │   └── style.css
│   └── js/
│       └── main.js
├── requirements.txt         # Dependencias del proyecto.
└── README.md                # Documentación de uso.
4.2. Diagrama de Componentes (Conceptual)
mermaid
Copiar
Editar
flowchart LR
    A[Sitemap.xml] --> B[sitemap_parser.py]
    B --> C[SQLite: existing_content.db]

    D[Seed Keywords] --> E[google_autocomplete.py]
    D --> F[google_paa.py]
    D --> G[google_trends.py]

    E --> E1[(autocomplete.csv)]
    F --> F1[(paa.csv)]
    G --> G1[(trends.csv)]

    C & E1 & F1 & G1 --> H[Fuzzy Matching & Análisis]
    H --> I[Flask App (app.py)]
    I --> J[Plantillas HTML (results.html)]
5. Flujo de Datos
Sitemap
Extrae URLs (y meta) → SQLite (tabla: content).
Scrape de Keywords
Google Autocomplete → autocomplete.csv
People Also Ask → paa.csv
Google Trends → trends.csv
Fuzzy Matching
Se comparan las keywords con existing_content.db (títulos/descripciones) para determinar “Cubierta / No cubierta”.
Visualización
En Flask, se muestran tanto las URLs existentes como las nuevas keywords, indicando su origen y estatus.
6. Diseño e Implementación
6.1. Módulo de Configuración (config.py)
Encargado de centralizar variables y ajustar parámetros (por ejemplo, SIMILARITY_THRESHOLD, RATE_LIMIT_DELAY, etc.). Puedes usar .env con python-dotenv para mayor flexibilidad.

6.2. Scrapers y Procesamiento
6.2.1. sitemap_parser.py

Función principal: parse_sitemap(sitemap_url).
Descarga sitemap.xml, extrae <loc> y, opcionalmente, scrapear la URL para obtener <title> y <meta name="description">.
6.2.2. google_autocomplete.py

Función: get_autocomplete_keywords(seed_keyword).
Usa la endpoint http://suggestqueries.google.com/complete/search?client=firefox&q=.
6.2.3. google_paa.py

Función: get_paa_keywords(seed_keyword).
Emplea Playwright en modo headless para obtener preguntas de "People Also Ask".
6.2.4. google_trends.py

Función: get_trends_keywords(seed_keyword).
Configura Pytrends (idioma, zona horaria) y obtiene “related queries”.
6.2.5. Fuzzy Matching

Función sugerida: match_keywords_with_content(keywords, content_rows).
Compara cada keyword con los títulos/descripciones usando, por ejemplo, difflib.SequenceMatcher.
6.3. Interfaz Web con Flask
app.py define rutas como:

/ (GET/POST): Para parametrizar el proceso (sitemap_url, seed_keywords, etc.).
/results (GET): Muestra la tabla con URLs y la tabla de keywords (origen + estatus).
/add_keyword (GET/POST): Permite añadir manualmente una nueva keyword.
Utiliza templates (index.html, results.html, etc.) y archivos estáticos (style.css, main.js) para presentar un dashboard básico.

6.4. Persistencia y Cache
SQLite:
Tabla content con campos url, title, description.
Archivos CSV:
Se guardan temporalmente las keywords de cada fuente (autocomplete.csv, etc.).
Caching:
Evita repetir scraping si ya se hizo recientemente (por ejemplo, guardando timestamps).
7. Manejo de Errores y Seguridad
Rate Limiting: time.sleep(RATE_LIMIT_DELAY) para prevenir bloqueos.
Rotación de User-Agents (opcional).
Excepciones: Manejo con try/except y registro en logs (logging module).
Localhost: Ejecutar preferiblemente en localhost para no exponer la app de scraping públicamente.
8. Pruebas y Validación
Extracción de Sitemap
Verificar que URLs se almacenan en existing_content.db.
Scraping de Autocomplete
Probar con keywords de ejemplo y confirmar la correcta extracción.
People Also Ask
Validar que se obtengan preguntas y se guarden en paa.csv.
Interfaz Web
Comprobar que /results muestre todos los datos (URLs, keywords, estado).
Fuzzy Matching
Testear con ejemplos conocidos para confirmar clasificación.
9. Despliegue y Operación
Local:
bash
Copiar
Editar
flask run --host=0.0.0.0 --port=5000
# Visitar http://localhost:5000/
Producción (Opciones):
Docker: Crear Dockerfile y desplegar en servicios como Railway, Render, etc.
Heroku: Procfile con web: python app.py y runtime.txt (Python 3.10+).
Netlify: No es el destino ideal para Flask, pero podría usarse con Netlify Functions + contenedor Docker.
10. Mejoras Futuras y Roadmap
Integrar NLP (por ejemplo, spaCy) para comparaciones semánticas más avanzadas.
Dashboard Interactivo con gráficos (Plotly, Chart.js) de tendencias.
Automatización con un cron job (Linux) o Task Scheduler (Windows).
Integración con CMS (WordPress, Drupal, etc.) para crear borradores automáticamente.
Cache Avanzado que evite completamente repeticiones de scraping durante un período (ej. 24-72 h).
11. Instrucciones para el "Developer IA"
En esta sección final, se detallan los pasos exactos que tu Developer IA (Windsurf, Codeium, etc.) puede seguir para generar el código en la secuencia lógica sin que tengas que intervenir manualmente:

Crear la Carpeta de Proyecto

Nombre sugerido: keyword_idea_generator.
Incluir un requirements.txt con las librerías indicadas.
Generar un Entorno Virtual (si aplica) y el Esqueleto de Flask

app.py con una ruta mínima @app.route('/') que devuelva "Hello World" para probar.
Configurar la Estructura de Directorios

scraper/, data/, templates/, static/, etc.
Crear sitemap_parser.py

Función: parse_sitemap(sitemap_url) que baje y parsee el XML del sitemap.
Crear Módulos de Scraping de Keywords

google_autocomplete.py con get_autocomplete_keywords(keyword).
google_paa.py con get_paa_keywords(keyword) usando Playwright.
google_trends.py con get_trends_keywords(keyword) usando Pytrends.
Implementar el Sistema de Persistencia

SQLite para almacenar contenido del sitemap (tabla content).
CSV en data/keywords/ para guardar las nuevas keywords.
Agregar Lógica de Fuzzy Matching

Por ejemplo, en un archivo core/fuzzy_match.py (o dentro de sitemap_parser.py) con la función match_keywords_with_content(...).
Diseñar las Rutas Flask en app.py

/ (GET/POST): formulario para ingresar el URL del sitemap y seed keywords.
/results (GET): muestra la lista de URLs y las keywords (cubiertas / no cubiertas).
/add_keyword (GET/POST): permite añadir una keyword manualmente.
Generar Plantillas y Archivos Estáticos

base.html (layout), index.html, results.html, add_keyword.html.
CSS (estilos básicos) y JS (lógica de frontend opcional).
Realizar Pruebas

Caso 1: Verificar extracción del sitemap.
Caso 2: Autocomplete con seed keywords.
Caso 3: PAA con Playwright.
Caso 4: Google Trends.
Caso 5: Fuzzy matching con contenido existente.
Documentar y Mejorar
Crear o actualizar el README.md con instrucciones de instalación y uso final.
(Opcional) Crear un Dockerfile si se desea desplegar en contenedores.
Resultado: Al completar estos pasos, tu IA generará los archivos necesarios y tu proyecto quedará totalmente funcional sin que toques manualmente el código.

12. Referencias y Recursos
Flask: Documentación Oficial
BeautifulSoup: Sitio Oficial
Playwright: Playwright para Python
Pytrends: Pytrends en GitHub
difflib: Documentación en Python
Despliegue en Docker: Documentación de Docker
Conclusión
